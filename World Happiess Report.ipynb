#!/usr/bin/env python
# coding: utf-8

# In[34]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'notebook')
import seaborn as sns
from warnings import filterwarnings
filterwarnings(action='ignore')


# In[35]:


data = pd.read_csv('https://raw.githubusercontent.com/dsrscientist/DSData/master/happiness_score_dataset.csv')
data.head(20)


# In[36]:


data.tail(10)


# In[37]:


data.shape


# In[38]:


data.isnull().sum()


# In[39]:


# We can see that there are no lack of values at all.
data.info()


# In[40]:


# Basic learning on my dataset
data.describe()


# In[41]:


# lets change the names of columns that include spaces between words to one word name
# This will make it easier for me to work on my data.
data= data.rename({'Happiness Rank':'happinessRank'}, axis='columns')
data= data.rename({'Happiness Score':'happinessScore'}, axis='columns')
data= data.rename({'Economy (GDP per Capita)':'economy'}, axis='columns')
data= data.rename({'Health (Life Expectancy)':'health'}, axis='columns')
data= data.rename({'Trust (Government Corruption)':'trust'}, axis='columns')
data= data.rename({'Dystopia Residual':'DystopiaResidual'}, axis='columns')
data.head()


# In[42]:


def HappinessLevel(happinessScore):
    if(happinessScore>=7):
        return 'Level1'
    if(happinessScore>=6):
        return 'Level2'
    if(happinessScore>=5):
        return 'Level3'
    if(happinessScore>=4):
        return 'Level4'
    if(happinessScore>=3):
        return 'Level5'
    if(happinessScore>=2):
        return 'Level6'
    else:
        return 'Level7'
    
data['HappinessScore'] = data.apply(lambda x: HappinessLevel(x['happinessScore']), axis=1)
data.HappinessScore.value_counts()


# In[44]:


labels = 'Level1', 'Level2', 'Level3', 'Level4', 'Level5', 'Level6'
sizes = [15, 29, 49, 44, 19, 2]
fig1, ax1 = plt.subplots()
ax1.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)
ax1.axis('equal') 

plt.show()


# In[45]:


regions=data.Region.unique()
for reg in regions:
    print(reg)


# In[46]:


region_counts= data['Region'].value_counts()
print(region_counts)


# In[47]:


region_counts= data['Region'].value_counts()
print(region_counts)


# In[48]:


region_values=region_counts.values
region_index=region_counts.index

plt.figure(figsize=(5,5))
sns.barplot(x=region_index,y=region_values)
plt.xticks(rotation=90)
plt.xlabel('Region')
plt.ylabel('Values')
plt.show()


# In[49]:


# We can see that most of the countries in my dataset are from Sub-Saharan Africa, while there are only two countries from North America and Australia and New Zealand.
sns.swarmplot(x="Region", y="happinessScore",  data=data)
plt.xticks(rotation=30)
plt.show()


# In[50]:


# correlation
plt.figure(figsize=(13,13)) 
sns.heatmap(data.corr(),annot=True,linewidth=.5)
plt.show()


# ## Money can't buy happiness - or can it?

# In[51]:


# Let's see the influence of Economy (GDP per Capita) on the Happiness Score.

dataA= data[["economy","happinessScore"]]
dataA


# In[53]:


sns.lmplot("economy","happinessScore",dataA)


# ### We can see that the connection between the economy and the happiness score is relatively linear. 
# ### In other words, the higher the economy, the higher the happiness score.

# In[54]:


data.economy.plot(kind='hist',bins=30,figsize=(8,8))


# In[55]:


# Let's see the influence of economy in the different parts of Europe.

europe1 = data[data.Region=='Western Europe']
europe2 = data[data.Region=='Central and Eastern Europe']
europe = pd.concat([europe1,europe2],axis=0)
europe.head()
sns.lmplot(data=europe,x='economy',y='happinessScore',hue="Region")


# ## Is health an important factor in our happiness?

# In[56]:


dataB= data[["health","happinessScore"]]
dataB


# In[57]:


data.happinessScore.mean()


# In[58]:


happy = data[data["happinessScore"] > 5.4] 
unhappy = data[data["happinessScore"] < 5.4]

happy.health.plot(kind = 'hist',bins = 50,figsize = (8,8), color = "green", label = "happy")
unhappy.health.plot(kind = 'hist',bins = 50,figsize = (8,8), color = "red", label = "unhappy")

plt.xlabel('Health')             
plt.legend()
plt.show()


# ### We can see that the vast majority of the "happy" countries having a high health rate. 
# 

# In[59]:


sns.lmplot("health","happinessScore",dataB)


# ### Here too, as in economics, the connection between health and happiness score is quite linear. 

# ## The extent to which Family contributes to the calculation of the Happiness Score
# 

# In[60]:


dataC= data[["Family","happinessScore"]]
dataC


# In[61]:


x= dataC.Family
y= dataC.happinessScore
plt.scatter(x, y, marker='o')


# ### We can notice that the "family" feature also has an effect on the happiness score. 
# ### When you look at the big picture you can say that as the family's ranking increases, so does the happiness score.

# ## Is freedom (to choose and decide in life) an important factor?

# In[62]:


dataD= data[["Freedom","happinessScore"]]
dataD


# In[63]:


data4 = pd.concat([data.Freedom, data.happinessScore],axis=1)
sns.pairplot(data4)
plt.show()


# In[64]:


plt.figure(figsize=(10,10))
sns.boxplot(x="Region", y="Freedom", data=data)
plt.xticks(rotation=70)
plt.title("Connection between region and freedom")
plt.show()


# In[65]:


# Let's see the influence of the four features I've studied so far on the happiness score (together):

plt.plot(data.happinessScore, data.economy, color = "red", label = "Economy")
plt.plot(data.happinessScore, data.Family, color = "yellow", label = "Family")
plt.plot(data.happinessScore, data.Freedom, color = "blue", label = "Freedom")
plt.plot(data.happinessScore, data.health, color = "green", label = "Health")

plt.legend()    
plt.xlabel('Happiness Score')           
plt.show()


# ### The extent to which Perception of Corruption contributes to Happiness Score

# In[66]:


dataE= data[["trust","happinessScore"]]
dataE


# In[67]:


x= dataE.trust
y= dataE.happinessScore
plt.scatter(x, y, marker='+')


# In[68]:


data_80MostHappy= data.head(80)
data_80MostHappy


# In[69]:


data_78LeastHappy= data.tail(78)
data_78LeastHappy


# In[70]:


plt.figure(figsize= (15,10))
sns.barplot(x= data_80MostHappy['happinessRank'], y= data['trust'], palette= sns.cubehelix_palette(80))
plt.xticks(rotation= 90) 
plt.xlabel('Happiness Rank')
plt.ylabel('Trust')
plt.title('Happiness Rank vs Trust')
plt.show()


# In[71]:


plt.figure(figsize= (15,10))
sns.barplot(x= data_78LeastHappy['happinessRank'], y= data['trust'], palette= sns.cubehelix_palette(78))
plt.xticks(rotation= 90) 
plt.xlabel('Happiness Rank')
plt.ylabel('Trust')
plt.title('Happiness Rank vs Trust')
plt.show()


# #### It seems that in the "happiest countries" trust is higher than in the "unhappiest countries" (The lines are longer on the first graph).
# 

# In[73]:


# Let's see the mean score: 
m= data_80MostHappy.trust.mean()
l= data_78LeastHappy.trust.mean()

print(m)
print(l)


# In[74]:


# Let's see the median score: 
from numpy import median
m2= data_80MostHappy.trust.median()
l2= data_78LeastHappy.trust.median()

print(m2)
print(l2)


# ### The last question was: Generosity.
# #### The willingness of people to contribute to others, and social support, which is the presence of someone who can be trusted in difficult times.

# In[75]:


dataF= data[["Generosity","happinessScore"]]
dataF


# In[76]:


x= dataF.Generosity
y= dataF.happinessScore
plt.scatter(x, y, marker='^')


# In[77]:


## # Let's see how Generosity is effect on Happiness in economically high and low countries.
x= sum(data.economy)/ len(data.economy)
x


# In[78]:


data["Economical_level"]= ["high" if i > x else "low" for i in data.economy]

h= data[data["Economical_level"] == "high"]
l= data[data["Economical_level"] == "low"]

plt.plot(h.Generosity, h.happinessScore, color = "red", label = "Economically High")
plt.plot(l.Generosity, l.happinessScore, color = "yellow", label = "Economically Low")
plt.xlabel('Generosity') 
plt.ylabel('Happiness Score')
plt.legend()

plt.show()


# ### We see that precisely in countries where generosity is greatest, the economy is low. 
# ### We can also clearly see that the economy is a much more significant and clear factor to happiness than generosity. 

# In[79]:


# Generosity and Freedom by Region
f,ax1= plt.subplots(figsize=(10,10))

sns.pointplot(x= "Region", y= "Generosity", data= data, color= "blue",alpha= 0.8)
sns.pointplot(x="Region", y="Freedom", data= data, color= "pink",alpha= 0.5)
plt.text(8,0.6,"Generosity",color= "blue",fontsize= 14)
plt.text(8,0.55,"Freedom",color= "pink",fontsize= 14)
plt.xticks(rotation= 80)
plt.ylabel("Happiness Score")
plt.grid()

plt.show()


# ## Summary of all features and their impact
# 
# ### 1.Economy (GDP per Capita) - red
# 
# ### 2.Health (Life Expectancy) - blue
# 
# ### 3.Family - green
# 
# ### 4.Freedom - yellow
# 
# ### 5.Trust (Government Corruption) - black
# 
# ### 6.Generosity - pink

# In[81]:


plt.scatter(data.happinessScore, data.economy, color = "red", label = "Economy")
plt.scatter(data.happinessScore, data.health, color = "blue", label = "Health")
plt.scatter(data.happinessScore, data.Family, color = "green", label = "Family")
plt.scatter(data.happinessScore, data.Freedom, color = "yellow", label = "Freedom")
plt.scatter(data.happinessScore, data.trust, color = "black", label = "Trust")
plt.scatter(data.happinessScore, data.Generosity, color = "pink", label = "Generosity")

plt.xlabel('Happiness Score') 
plt.legend()
plt.title('Happiness Score Relations')    

plt.show()


# ### The residuals, differ for each country, reflecting the extent to which the six variables either over- or under-explain average 2012-2014 life evaluations. These residuals combined with the estimate for life evaluations in Dystopia so that the combined bar will always have positive values.

# In[82]:


dataG= data[["DystopiaResidual","happinessScore"]]
dataG


# In[83]:


# The extent to which Dystopia Residual contributed to the calculation of the Happiness Score.

x= dataG.DystopiaResidual
y= dataG.happinessScore
plt.scatter(x, y, marker='*')


# In[84]:


sns.lmplot("DystopiaResidual","happinessScore",dataG)


# ## Lets find a model that will best predict the happiness score of a country
# 

# ## Model Training

# ### LINEAR REGRESSION

# In[86]:


feature_cols = ['economy', 'health', 'Family', 'Freedom', 'trust', 'Generosity']
X = data[feature_cols]

X.head()


# In[87]:


print(type(X))
print(X.shape)


# In[88]:


y= data.happinessScore

print(y.head())
print(y.tail())


# In[89]:


print(type(y))
print(y.shape)


# In[90]:


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)


# In[91]:


print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)


# In[92]:


from sklearn.linear_model import LinearRegression
linreg = LinearRegression()

linreg.fit(X_train, y_train)


# In[93]:


print(linreg.intercept_)
print(linreg.coef_)


# In[94]:


list(zip(feature_cols, linreg.coef_))


# In[95]:


y_pred = linreg.predict(X_test)

print(y_pred)
print(y_test)
print("test:")
print(linreg.score(X_test,y_test))
print("train:")
print(linreg.score(X_train, y_train))


# In[96]:


from sklearn import metrics

# Mean Absolute Error (MAE)
print(metrics.mean_absolute_error(y_test, y_pred))


# In[97]:


# Mean Squared Error (MSE)
print(metrics.mean_squared_error(y_test, y_pred))


# In[98]:


# Root Mean Squared Error (RMSE)
print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))


# In[99]:


# I have seen before that the feature "DystopiaResidual" also has an effect on the happiness score.
# So maybe my model will be better with it...
# Let's cheak it out (and check the RMSE)!

feature_cols = ['economy', 'health', 'Family', 'Freedom', 'trust', 'Generosity', 'DystopiaResidual'] 
X = data[feature_cols]

y= data.happinessScore

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

linreg.fit(X_train, y_train)

y_pred = linreg.predict(X_test)

print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))


# In[100]:


print(linreg.intercept_)
print(linreg.coef_)


# In[102]:


list(zip(feature_cols, linreg.coef_))


# In[103]:


print("test:")
print(linreg.score(X_test,y_test))
print("train:")
print(linreg.score(X_train, y_train))


# In[104]:


# Mean Absolute Error (MAE)
print(metrics.mean_absolute_error(y_test, y_pred))


# In[105]:


# Mean Squared Error (MSE)
print(metrics.mean_squared_error(y_test, y_pred))


# In[106]:


# Root Mean Squared Error (RMSE)
print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))


# ### The RMSE decreased when we added "DystopiaResidual" to the model! 
# ### The conclusion: This is a very important feature for the model!

# In[107]:


# plotting the model 

plt.scatter(y_test, y_pred)
plt.xlabel('True Values')
plt.ylabel('Predictions')


# ### The high score (99.9%) I got before could be result in overfitting, what we’re actually trying to avoid! 
# ### This is where cross validation comes in.

# ## K-Folds Cross Validation
# 
# 

# In[108]:


from sklearn.model_selection import KFold

kf= KFold(n_splits=5).split(range(25))


# In[109]:


#for train_index, test_index in kf.split(X):
 #   print(“TRAIN:”, train_index, “TEST:”, test_index)
  #  X_train, X_test = X[train_index], X[test_index]
   # y_train, y_test = y[train_index], y[test_index]
    
print('{} {:^61} {}'.format('Iteration', 'Training set observations', 'Testing set observations'))
for iteration, data in enumerate(kf, start=1):
    print('{:^9} {} {:^25}'.format(iteration, data[0], str(data[1])))


# In[110]:


from sklearn.model_selection import cross_val_score

scores = cross_val_score(linreg, X, y, cv=10)
print(scores)
print("scores mean:", scores.mean())


# ##  Ridge Regression
# 

# In[111]:


from sklearn.linear_model import Ridge

rid = Ridge(alpha=0.5)
print(rid)


# In[112]:


rid.fit(X_train, y_train)


# In[113]:


c= rid.coef_
i= rid.intercept_

print("coaf:", c)
print("intercept:", i)


# In[114]:


list(zip(feature_cols, rid.coef_))


# In[115]:


y_pred= rid.predict(X_test)
y_pred


# In[116]:


print("test:")
print(rid.score(X_test,y_test))
print("train:")
print(rid.score(X_train, y_train))


# In[117]:


scores = cross_val_score(rid, X, y, cv=10)
print(scores)
print("scores mean:", scores.mean())


# In[118]:


# search for an optimal value of the alpha parameter. 

a_range = list(range(0, 20))
a_scores = []
for a in a_range:
    rid = Ridge(alpha=a)
    scores = cross_val_score(rid, X, y, cv=10)
    a_scores.append(scores.mean())
print(a_scores)


# In[119]:


# RidgeCV implements ridge regression with built-in cross-validation of the alpha parameter.

from sklearn import linear_model
alphas= list(range(0, 20))
reg = linear_model.RidgeCV(alphas=alphas, cv=3)
reg.fit(X_train, y_train)   


# In[120]:


reg.alpha_  


# In[121]:


# I have received that the optimal value for alpha is 0. 
# When a=0 The objective becomes same as simple linear regression. We’ll get the same coefficients as simple linear regression.
# Conclusion: The model of linear regression is the best. 


# In[122]:


# plot the value of a for Ridge (x-axis) versus the cross-validated accuracy (y-axis)
plt.plot(a_range, a_scores)
plt.xlabel('Value of alpha')
plt.ylabel('Cross-Validated Accuracy')


# ### As the alpha value increases, the accuracy of the model is reduced.
# 

# ## Summary:
# ### The model of linear regression is the most successful for my problem.
# ### I reached a level of accuracy of 99.9 percent!
# ### All of the features that appear in my DataSet affect the happiness score in a rising connection, and probably linear.

# In[ ]:




